%!TEX program = xelatex 
% 告诉 texstudio， 无论默认编译器什么设置， 本 tex 文档都采用 xelatex 编译
% 参考：https://github.com/YimianDai/iNSFC

% 问题：参考文献为问号
% 解决：配置bibtex
% D:/matlab/latex/bin/win32/bibtex.exe

% 神经代码：F1205

% 科研处：http://kyc.hutb.edu.cn/show_content.php?id=5559


\documentclass[a4paper,zihao=-4]{article}
\usepackage[UTF8,punct,linespread=1.56]{ctex}
% \documentclass[a4paper,cs4size,UTF8,punct,linespread=1.56]{ctexart}
\pagestyle{empty} % 第二页以后页码空白
\usepackage[a4paper, left = 3.2cm, right = 3.2cm, top = 2.54cm, bottom = 2.54cm]{geometry}

% 解决下划线不换行
\usepackage{ulem}
\usepackage{CJKulem}

\usepackage{xcolor}
% \usepackage[citebordercolor = white]{hyperref}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage[subrefformat=parens,labelformat=parens]{subfig} %
\usepackage{booktabs} % for \toprule \midrule \bottomrule \cmidrule
\usepackage{cleveref}
\crefformat{table}{表~#2#1#3}
\crefformat{figure}{图~#2#1#3}
\crefformat{equation}{式~（#2#1#3）}
\usepackage{enumitem}
% \setenumerate[1]{itemsep = 0pt, parsep = 0pt, topsep = 2bp}
\setlist[enumerate]{itemsep = 0pt, parsep = 0pt, topsep = 2bp}
% \setitemize[1]{itemsep = 0pt, parsep = 0pt, topsep = 2bp}
\setlist[itemize]{itemsep = 0pt, parsep = 0pt, topsep = 2bp}
\usepackage{fontspec}
\setmainfont{Times New Roman}
% \usepackage{minted}   % For syntax highlighting
% \usemintedstyle{friendly}
\usepackage{setspace} 
\usepackage{caption}
\DeclareCaptionFont{capfont}{\kaishu\zihao{-4}\selectfont} % Caption font
\DeclareCaptionFont{subfont}{\kaishu\zihao{5}\selectfont} % Sub-caption font
\captionsetup{font = capfont}
\captionsetup[subfigure]{font = subfont}
\captionsetup[figure]{labelsep=space} % 空格 space；点 period；冒号 colon
\captionsetup[table]{labelsep=space}  % 空格 space；点 period；冒号 colon
\usepackage[square,numbers,sort&compress]{natbib}   % For Reference
\newcommand{\citess}[1]{\textsuperscript{\cite{#1}}}
\setlength{\bibsep}{1pt plus 0.3ex}
\usepackage{titlesec}
\titleformat{\subsubsection}[block]{\hspace{3em}}{\thesubsubsection}{1em}{}

% 使用.sty文件(insfc.sty)来单独存放常用格式设置，可避免每次书写.tex文档时重复书写类似导言
\usepackage{insfc}



\graphicspath{{images/}}   % 设置图片所存放的目录

\begin{document}

\kaishu

% Decrease space above and below equations
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}

%%%%%%%%% TITLE %%%%%%%%%
% \title{报告正文 \vspace{-3.4ex}}
% \title{报告正文}
% \maketitle
\begin{center}
	{\kaishu \zihao{2} \textbf{报告正文} \vspace{-3ex}}
\end{center}  

\thispagestyle{empty} 　　　% 首页页码空白

%%%%%%%%% Your Content %%%%%%%%%
%{\kaishu \zihao{4}参照以下提纲撰写，要求内容翔实、清晰，层次分明，标题突出。}\alert{请勿删除或改动下述提纲标题及括号中的文字。\vspace{9bp}}

\NsfcChapter{（一）立项依据与研究内容}{（建议8000字以内）：}

\NsfcSection{1}{项目的立项依据}{（研究意义、国内外研究现状及发展动态分析，需结合科学研究发展趋势来论述科学意义；或结合国民经济和社会发展中迫切需要解决的关键科技问题来论述其应用前景。附主要参考文献目录）；}

\textcolor{NsfcBlue}{\subsection{研究意义}}
%\NsfcSection{研究意义}
近年来，人工智能技术发展突飞猛进，并以不可阻挡之势影响着社会和人们的生活。
许多国家和地区都将人工智能的发展作为提高国家实力、增强核心竞争力和建设现代化强国的核心所在。
例如，2020 年 3 月，中共中央政治局常委会提出将人工智能作为新型基础设施建设七大板块中的重要一项，以便更好地推动中国经济的转型和升级。
2021 年 3 月，美国国家人工智能安全委员会发布了《人工智能国家安全委员会最终报告》，提出推动人工智能以及相关技术的研究和应用，以解决国家安全和国防需求~\cite{schmidt2021national}。
而计算机视觉作为人工智能最重要的领域之一，正在成为科学研究和产业落地的重要方向。

基于深度神经网络的计算机视觉模型已经取得了巨大的进步~\cite{ILSVRC15}，然而随之而来的是越来越复杂的模型和可解释性的缺失，导致单目标跟踪算法难以扩展到多目标场景，并严重制约了模型的理解与实际应用。
尽管更深的深度神经网络确实能提高跟踪的精度，但这并不能提高神经网络模型和人脑的相似性~\cite{rajalingham2018large}。
深度学习发展初期部分人工神经网络模块可以映射到大脑皮层视觉通路的相应区域，然而随着模型的发展，比如 GoogleNet~\cite{szegedy2015going} 或 Inception-v4~\cite{szegedy2017inception}中，人工神经网络中众多复杂模块与大脑皮层中少量的视觉区域之间的关系越来越弱。
为了在模型中更准确地捕捉大脑皮层的处理模式，提高模型的可解释性和降低模型复杂性，仅仅基于传统视觉数据集进行模型架构搜索似乎不再是可行的解决方案。

在目标识别领域，深度模型在构建具有神经可解释的模型方面取得了一些成就~\cite{RN687}，
出现了一些设计类脑深度神经网络架构用于图像识别的工作~\cite{RN1115, RN688}。
其中的神经元能部分解释为什么人类大脑视觉皮层中的神经细胞对图像有特定的激活~\cite{RN1127,RN1144,RN1145,RN1535,RN1146,RN1100}。
这些模型也部分预测了灵长类图像分类的行为和评估~\cite{RN730,RN1139}，提高模型可解释性的同时降低了模型复杂度。
同时这些优秀的类脑模型能预测在人类大脑皮层通路中产生的激活模式~\cite{RN968}，为脑机接口的实现提供了很好的机会。

同时，随着人工智能的蓬勃发展，以图像分类、视觉目标跟踪、目标检测等任务为代表的计算机视觉领域也获得了广泛的关注并在理论研究和实际应用方面取得了巨大的进展。
作为人工智能与计算机视觉领域最重要的问题之一，视觉感知在智能驾驶系统、服务机器人和无人机等方面有着十分重要的应用。
随着深度学习和传统机器学习相关理论的发展与完善，封闭固定环境下的视觉感知已经达到了实际应用所需的性能要求。
一些面向封闭环境的视觉感知系统已经被用于现实生活中以保证人们的生命财产安全，比如家庭智能监控系统等。
与封闭环境下的视觉感知相比，开放环境下的视觉感知具有更为重要的应用价值和社会需求。
例如，对于智能驾驶系统来说，视觉感知功能是其完成智能决策所必不可少的组成部分。
一个稳定且可靠的智能驾驶系统必须在开放环境中准确地感知到智能车周边的不同目标（如车辆、行人等）以便做出合理的决策来规避潜在的交通风险，进而保证智能车的行车安全。
然而，开放环境下的视觉感知是一个非常具有挑战性的任务并且依然存在着诸多技术难点和挑战，这极大地限制了各类视觉感知方法的感知性能和在实际工程中的推广与使用~\cite{RN308}。
因此，深入研究面向开放环境的鲁棒视觉感知方法具有非常重要的理论研究意义和实际应用价值。
针对开放环境下的视觉感知任务，已有许多机器学习方法~\cite{smeulders2013visual}和深度网络模型\cite{li2018deep}被研究以进一步提高感知性能。
但是由于某些技术困难与挑战，这些方法无法获得理想的感知精度来完成从封闭环境到开放环境的有效跨越，也不能被真正地用于解决各种实际问题。
具体来说，开放环境下的感知主要存在以下几个方面的难点和挑战：

1）\textbf{视觉感知中的模型可解释性问题}。
随着深度学习在实际视觉场景中中使用的越来越多，日益复杂的深度模型时间复杂度高、训练和推理速度慢，无法用于实际场景。
%
一个完整的视觉感知系统主要包括目标识别和目标跟踪两个基础任务，其中目标识别主要用于提取区分不同目标的表征，目标跟踪则主要负责在视频序列中确定目标位置。
因此，为了追求更好的感知性能，主流的感知方法往往会学习到高度复杂的视觉感知模型。
以目前十分流行的目标的特征提取模型包括 vgg19~\cite{simonyan2014very}、resnet~\cite{he2016deep}、googlenet~\cite{szegedy2015going}等。
高度复杂的特征提取模型会极大增加模型的训练时间和推理时间，从而降低视觉感知方法的应用能力。
例如，在智能驾驶系统中，高度复杂的视觉感知模型将极大地降低感知方法的适应性，从而造成视觉感知方法不能在复杂的交通场景中准确地感知到智能车周围的重要目标以规避交通风险。
机器学习中的奥卡姆剃刀原则（Occam’s Razor）指出~\cite{ml_zhou}：在所有可以选择的模型当中，能够很好地解释已知数据并且十分简单才是最好的模型。
同时，凭借经验设计深度网络模型导致模型的可解释性能力差，极大的限制了视觉感知模型在现实场景中的推广和应用。
因此，如何降低视觉感知模型复杂度和提升模型的可解释性，以提高复杂场景下的感知适应性是开放环境下视觉感知的一个重大挑战。


2）\textbf{环境和目标动态变化}。
在动态开放环境下，目标自身会发生十分复杂的外观变化，这极大地增加了跟踪方法取得理想性能的难度。
目前主流的 OTB 数据集\cite{wu2013online}和每年举办的 VOT（Visual Object Tracking）\cite{kristan2019seventh}目标跟踪竞赛都把目标的外观变化分为多种类型，如：运动模糊、目标形变、光照变化、平面旋转等。值得一提的是，目标在某个时刻可能同时发生多种类型的外观变化，这将直接导致目标出现非常复杂的外观变化，进而限制跟踪方法在开放环境下的跟踪准确性。
能否成功地应对目标复杂的外观变化将决定跟踪方法的跟踪性能和在实际问题中的应用前景。
以刑事侦查中所依靠的天眼系统为例，如果目标在跟踪过程中所出现的复杂外观变化能够被有效地捕获，那么天眼系统将顺利地完成跟踪任务以确保社会安全，反之则系统可能在某一时刻跟丢目标，后果将十分严重。
综上所述，如何捕获目标所出现的复杂外观变化以提高跟踪准确性是开放环境下目标跟踪的又一个重大挑战。

3）\textbf{单一模态感知信息不足问题}。
在视觉感知过程中，场景中不仅有视觉信息，而且还会包含其他模态的信息。
因此，理想的感知模型必须综合考虑各种模态的输入信息以应对复杂场景下鲁棒性问题。
然而，目前主流的视觉感知方法依然不具备良好的多模态融合感知鲁棒性，实验室条件下的模型迁移到真实场景时经常会出现感知能力不足的问题，进而导致应用失败的结果。
对于ADAS系统（Advanced Driving Assistance System，高级驾驶辅助系统）和天眼系统等应用系统来说，能否综合利用其他模态的信息也在很大程度上决定着这些系统的性能和应用前景。
所以理想情况下能融合其他模型的信息，将在一定程度上提高模型的感知能力。
因此，如何解决单一模态感知信息不足的问题以保证跟踪鲁棒性也开放环境下感知的一个重大挑战。


针对上述挑战，本项目从模型的可解释性和精简压缩、环境和目标动态变化、单一模态信息感知能力不足等方面展开研究，
基于大脑皮层解剖对齐思想，解决开放环境下目标跟踪所面临的多个关键性科学问题，实现开放场景下的鲁棒视觉感知。
具体的研究工作包括：

（1）拟提出基于腹侧流解剖对齐的类脑精细化识别模型，主要解决由于跟踪模型复杂度过高和解释能力不足所引起的跟踪适应性问题；

（2）拟提出基于腹侧流和背侧流融合的类脑视觉跟踪模型，重点解决目标所出现的复杂外观变化以提高开放场景下的跟踪准确性；

（3）拟提出基于大脑皮层解剖对齐和全局工作空间的多模态融合模型，主要解决视觉感知过程中单一模态信息感知能力不足的问题以提高开放场景下的跟踪鲁棒性。
值得一提的是，由于上述三个工作分别从图像识别、目标跟踪和多模态融合角度出发以解决关键科学问题，因此工作（1）可以作为（2）的基础，同时（1）和（2）可以直接嵌入到工作（3）中以保证研究的整体性和统一性。
项目基于前期关于注意力机制\cite{nai2018robust,li2018visual}、类脑跟踪~\cite{nai2019multi,li2020reliable}、多任务学习等方法的研究基础，结合神经科学、多模态融合和全局工作空间等相关方法和理论，
拟提出基于腹侧流解剖对齐的类脑精细化识别模型、基于腹侧流和背侧流融合的类脑视觉跟踪模型、基于大脑皮层解剖对齐和全局工作空间的多模态融合模型，
解决开放场景下视觉感知任务中的可解释性、环境和目标动态变化挑战、单一模态信息感知能力不足等重要问题，
提高感知方法的视觉感知性能，实现开放环境下的类脑鲁棒视觉感知方法。
本课题具有十分重要的理论研究意义和实际应用价值，相关研究成果将为视觉感知的发展提供重要参考，并促进视觉感知方法在不同领域中的现实应用。


\textcolor{NsfcBlue}{\subsection{国内外研究现状及发展动态分析}}
\subsubsection{类脑目标识别研究现状}
% 参考：https://cloud.tencent.com/developer/article/1626780
视觉感知是人工智能领域最重要和最有挑战性的任务之一，具备十分重要的科学价值和应用价值。
近年来，许多视觉感知算法被提出以便在开放环境下取得理想的感知性能。
总的来说，视觉感知方法的基础任务包括两个：目标识别算法和目标跟踪算法。
而类脑识别方法作为一个新兴的方法，有解决模型可解释问题的潜力。

Kamitani等~\cite{horikawa2017generic}提出的基于深度学习的神经解码方法，研究了各种视觉特征与大脑视觉皮层各个脑区之间的对应关系，分析了计算机视觉模型和人脑视觉之间的联系和差异。
视觉编码模型~\cite{RN1519}要求通过模拟大脑视觉系统信息处理过程，预测不同外部图像刺激下的视觉皮层响应。
视觉信息解码模型~\cite{decoding_fMRI}能够根据视觉皮层响应预测响应自然图像刺激的内容，如图像目标语义类别、场景内容等。

张铁林等~\cite{zhang2018brain}提出的脉冲神经网络更新方法同时考虑了神经元的四个特性，包括神经元膜电位总是趋于稳态，神经元存在兴奋型和抑制型两类，神经元突触拥有短时可塑性，神经元之间存在逆向通路用于信号的反响传播，利用这四种特性，就可以使用简单的脉冲神经网络网络结构获得较好的图像分类结果，该研究既能保证脉冲神经网络仿生特性不减少，又能大幅提高图像分类准确率。
Naselaris等人\cite{st2018feature}提出的基于深度特征加权的神经编码模型，该研究基于群体感受野和多层特征加权思想构建了一个可解释的神经编码模型，相较于传统的编码模型，该方法效果更好，可解释性也更好。

% 
但由于学习得到的识别模型具有很高的复杂度，这些方法的识别速度和可解释性都会受到很大的影响。
由上可知，模型参数量更少和模型结构具有更好的解释性可以取得比其他方法更优的识别性能，具有更好地应用前景。
然而，在寻求更好的识别结果的同时，这些方法往往学习到高度复杂的识别模型。
当使用深度特征时，该类方法的模型复杂度将变得不可接受。
高度复杂的识别模型严重降低了识别方法对复杂场景的适应性。
因此，提出有效方法降低跟踪模型复杂度以提高开放场景下的识别适应性具有重要意义。


\subsubsection{视觉目标跟踪研究现状}

Zhongming Liu等~\cite{wen2018neural}提出的基于深度学习的神经编解码研究，该研究从动态视频刺激入手研究人脑对动态场景的编码和解码能力。采用 AlexNet 为基础深度学习网络，将视频刺激在 AlexNet 中的各层表征与大脑视觉皮层各个脑区建立关系，得出初级视觉皮层与神经网络的浅层特征相关性较高，而高级视觉皮层与神经网络深层次特征相关性较高的结论。
Horikawa等~\cite{horikawa2020neural}等证明利用动态视频诱发情绪在神经表征上是高维的、分类的，并且分布在跨模式的大脑区域中。
Shrestha等\cite{shrestha2018slayer}利用概率的方法解决了脉冲信号不可求导的问题，从而大幅提高了脉冲神经网络用于图像分类的准确率，还创新性地将脉冲神经网络用于动态图像识别、人体动作识别和音频识别，获得了非常好的实验效果。


通常使用人工合成刺激来研究人眼凝视，比如研究眼跳采用改变目标位置时眼睛注视的形式，而研究平滑跟踪则使用线性或者正弦运动的点。
使用人工合成数据最大的优势是拥有定义良好的属性和明显特征，特定的特征简化了眼球跟踪和血氧水平依赖信号的分析的过程。通过仅仅沿着一个独立的特征位置，可以进行理想的刺激调制，能更加精确地建立起特定脑区之间激活的联系。
但这些优势所牺牲的是人工合成数据并不能代表人类正常的视觉，实际有效的视觉输入会更加复杂，并且动态开放的环境中平滑跟踪不会单独出现，而是和眼跳和注视的序列混在一起。
因此，在单一背景上的人工合成刺激忽略了可能的背景信息、拥挤效应和全部眼睛运动规划流程的影响。
另外一个缺点是长时间的跟踪单一背景下的人工合成刺激会导致人注意力维持的降低。

特别是日常生活中，在动态背景上进行平滑跟踪，并行处理互相冲突的信息通常是必要的，这种情况下可能在不同方向上包含多个移动的目标。
值得注意的是从猴子的研究中得出结论：V5 不仅在平滑跟踪中，而且即使在视网膜上冲突时不同刺激之间的相互作用中也发挥重要的作用~\cite{neural_sp}。
然而，在自然条件下处理平滑跟踪眼睛运动所引起的动态视觉输入时必须非常小心。
然而即使是对于非常大且多样的数据集，最近计算机视觉领域所取得的进步已经开始能够自动理解动态开放的场景。
相比于人工合成的刺激，当使用不受限的动态自然刺激，在实验中如何利用运动眼睛的显示指令（比如：“跟踪这个点”、“当目标出现时进行眼跳”等），将凝视轨迹分成不同眼睛运动成分仍然很有挑战。
手动标注“真实数据”被认为是眼睛运动成分分类的黄金标准，对于每一秒的凝视信号，只有注视和眼跳能在4 到15 秒的时间内到达场景的任何地方~\cite{gold_standard}。
比如，StudyForrest 数据集中提供了大约30 小时的fMRI 和同步的凝视记录~\cite{gaze_forrest}。
因此，StudyForrest 数据集同时记录了视觉和非视觉线索相关的脑部激活，可以用来更好的进行眼睛运动成分的分类。


% 参考：https://blog.csdn.net/weixin_43741711/article/details/106071445
\subsubsection{多模态融合研究现状}

由于环境的复杂性，单一视觉数据所提供的信息已经不能满足机器认知能力的需求，与人类利用视觉、听觉、触觉等多种感官信息来感知世界类似，机器也需要模拟人类联觉来提升认知水平。
为了确保感知方法的鲁棒性，必须设计有效的方法解决视觉感知单一模态不足的问题。
其他模态的数据可以是音频、文本等描述同一对象的多媒体数据。
常见的信息融合方式有物理层融合、特征层融合、决策层融合几个类型。
物理层融合指在传感器层级对采集到的数据进行融合处理，即多传感器信息融合，在工业生产场景中最常见的信息融合方法。
特征层融合指在特征抽取和表达的层级对信息进行融合，例如对不同摄像头采集到的图像数据采用相同的特征表达形式，进而进行相应的叠加计算。
决策层融合指对不同模态的感知模型所输出的结果进行融合，这种融合方式对传感器性能和种类要求相对不高，但具有较大的信息损耗。

% 类脑其他模态(音频)
AVLnet\cite{rouditchenko2020avlnet}和PolyViT\cite{likhosherstov2021polyvit}融合了图像、视频和音频的的特征。
Jing Huang~\cite{huang2013audio}等使用深度信念网络进行视听语音识别，并使用来自音频和视觉特征的深度学习来进行噪声鲁棒语音识别。
CLIP~\cite{radford2021learning}可以有效地从自然语言监督中学习视觉概念。
Gota\cite{reed2022generalist}实现了多模型、多任务、多具象的多面手智能体。

% 全局工作空间
全局工作空间理论的核心主张是，心智状态在全局工作空间内广播时具有意识，额叶与顶叶区域的神经网络在全局工作空间中扮演着中心枢纽般的角色。
当局部处理系统（例如感觉区域）被点火时，其中的活动会暂时性地具有对工作空间的“可移动性”\cite{dehaene2001towards}。
对全局工作空间理论的实验证据支持包括，将意识与点火的神经元特征信号，或长程信息共享的神经元特征信号相关联的研究~\cite{dehaene2011experimental,mashour2020conscious,demertzi2019human,van2018threshold}。
在外界刺激发生约200-300ms之后，对应于有意识状态或无意识状态的试次，前部皮层区域的神经活动具有差异，这种差异被认为是“点火”的神经元特征信号。
这类实验包括“无报告范式”实验\cite{sergent2021bifurcation}（另见\cite{sergent2005timing}）。
这种研究最近已推广到解码领域。
例如，刺激后约300ms的神经活动模式能被用来预测主观报告，而且这种方法可在不同感知模态间泛化\cite{sanchez2020decoding}。
有意识或无意识的内容的长程信息传递特征已经使用一系列方法进行了鉴定\cite{mashour2020conscious,gaillard2009converging}。
意识知觉的第一个标志是双侧前额叶和顶叶强烈的放电活动。
意识知觉的第二个标志是正向慢波（P3波），有意识的知觉起始于270毫秒左右，在430毫秒时左右额叶和前扣带回中出现P3a波，580毫秒时活动又回到后侧的视觉区的P3b波（回归波）。
% 回归波也许是维持视觉表征的记忆（一瞬间的意识不能保存）
意识知觉的第三个标志是在刺激呈现大约300毫秒的时间里，$ \gamma $ 波大幅增加。
意识知觉的第四个标志是跨越整个皮质的巨大同步信号。
这些意识知觉的标志需要新的工具来理解这些复杂的模式，比如深度学习。

%
为了获得满意的融合性能，基于多模态融合的感知方法需要满足两个基本要求：
首先，多模态融合方法必须学习具有多样性的特征提取器以确保最终的特征提取器能够应对复杂多变的感知场景；
其次，所有特征提取器必须以合理的方式进行融合以保证融合特征能够对环境进行全面系统的感知。
然而，现存的基于多模态融合的感知方法无法很好的满足上述两个基本要求，这严重地限制了感知的鲁棒性和准确性。
由上可知，多样的类脑特征提取器和合理的融合策略是类脑多模态融合方法取得理想感知性能的关键，
前者保证感知方法可以成功应对感知过程中多变的场景，
后者则能确保感知方法能有效利用各种模态信息的。
设计高效的类脑多模态融合感知方法来消除单一模态感知能力不足的问题和对提高开放环境下感知的鲁棒性具有重要意义。


% 参考文献放这里
\begin{spacing}{1.3} % 行距
	\zihao{5} \songti   
	\bibliographystyle{gbt7714-nsfc}
	\bibliography{D:/BaiduSyncdisk/work/paper/doctor,ref}  
	\vspace{11bp}
\end{spacing}



\NsfcSection{2}{项目的研究内容、研究目标，以及拟解决的关键科学问题}{（此部分为重点阐述内容）；}



% 实现意图的判断，危险的规避
% 面向开放场景的鲁棒类脑视觉感知方法研究
% 机理、模型、应用
% 基于大脑皮层解剖对齐的类脑鲁棒视觉感知方法研究
% 科研处
\textcolor{NsfcBlue}{\subsection{研究目标}}
本项目以开放环境下的视觉感知所面临的\textbf{模型可解释低}、\textbf{环境和目标动态变化}和\textbf{单一模态信息不足等挑战出发}，
从解剖对齐的\textbf{类脑模型设计}、\textbf{运动感知}和\textbf{多模态融合框架设计}等方面展开研究，
聚焦深度学习、类脑模型、目标识别、视觉跟踪和多模态融合等相关理论与方法，
\textbf{重点突破基于大脑皮层解剖对齐的类脑精细化识别模型}、\textbf{基于大脑解剖对齐的类脑跟踪模型}、\textbf{基于大脑皮层解剖对齐的多模态融合模型等关键技术}，
研发面向开放环境的类脑鲁棒视觉感知算法，并利用\textbf{标准视频和神经记录数据集}、\textbf{ADAS高级智能驾驶辅助系统}和\textbf{真实智能车平台}相结合的实验手段进行算法验证与测试，
提高开放环境下的感知适应性、准确性和鲁棒性，
设计具有良好性能的视觉感知系统，推动类脑感知在实际问题中的具体应用，为实现开放环境下的类脑鲁棒视觉感知提供新技术和新方法。
图~\ref{fig:research}~展示了本项目的技术挑战、研究内容和关键科学问题。

% 字多
\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/research"}
	}
	\caption{ 本项目整体研究思路与关系}
	\label{fig:research}
\end{figure*}


\textcolor{NsfcBlue}{\subsection{研究内容}}

\subsubsection{基于大脑解剖对齐的类脑精细化识别模型}
本项目针对开放环境下视觉感知所存在的模型可解释性问题，从类脑模型设计和精细化识别的角度出发，拟提出基于大脑解剖对齐的类脑精细化识别模型以限制跟踪模型复杂度，提升跟踪方法在开放环境下的可解释性和适应性。

% 机理、模型、应用
\begin{itemize}
	\item 掌握大脑识别机理方面的理论与方法，深入研究皮层识别脑区和类脑模块（例如：卷积神经网络和循环神经网络等）的识别模型对识别鲁棒性和可解释性的影响，了解类脑模型方法在各类计算机视觉任务中的具体应用；
	\item 将大脑解剖对齐的模型设计思想引入到类脑精细化识别模型当中，设计基于大脑结构约束的类脑识别模型，研究高效的目标识别算法以正确地感知视觉目标；
	\item 研究类脑模型各层的激活和大脑皮层对应脑区激活之间的相似性，设计相似性度量标准全量衡量中间层激活和人类识别行为选择，判断所提出模型的有效性和鲁棒性；
\end{itemize}

\subsubsection{基于腹侧流和背侧流融合的的类脑跟踪模型}
本项目针对目标在开放环境下所出现的复杂外观变化，从腹侧流识别和背侧流运动感知相互融合的角度出发，拟提出基于腹侧流和背侧流融合的类脑跟踪模型以有效地捕获目标所出现的复杂外观变化，提高视觉目标跟踪方法的可解释性、鲁棒性和准确性。

\begin{itemize}
	\item 研究循环等结构对人类和深度网络在视觉跟踪中的作用机理，分析腹侧流和背侧流在人类视觉目标跟踪中发挥的作用，以及不同结构和深度特征在不同跟踪场景下的跟踪性能；
	\item 利用腹侧流和背侧流的解剖结构和作用机理，设计基于解剖对齐的类脑鲁棒跟踪模型，并衡量类脑深度模型和大脑皮层在激活相应和跟踪行为输出之间的相似性；
	\item 在上述基础之上，研究类脑跟踪模型对腹侧流和背侧流相互作用的理解，提高跟踪的可解释性和准确性；
\end{itemize}


\subsubsection{基于大脑皮层解剖对齐的多模态融合感知模型}
本项目针对真实场景视觉感知过程中所出现的信息不足等问题，从感知框架的角度出发，拟提出基于大脑皮层解剖对齐的的多模态融合感知方法以解决特征不足的问题，提高感知模型在开放环境下的鲁棒性。

\begin{itemize}
	\item 研究和分析全局工作空间在类脑多模态融合感知模型中的应用，分析模型对感知性能的影响，以应对模型信息不足问题，提高模型在开放场景下的鲁棒性；
	\item 设计大脑皮层解剖对齐的类脑音频感知模型和音视频特征融合模型；
	\item 研究与探索类脑多模态融合感知方法在意图判断等方面的应用，进一步提高类脑感知方法在复杂场景下的鲁棒性；
\end{itemize} 

综上所述，本项目旨在从模型可解释性、目标动态变化和融合框架设计等角度展开研究以设计具有鲁棒性的类脑视觉感知方法。
考虑到视觉感知系统中目标跟踪需要提取鲁棒的识别特征，所以研究内容一是研究内容二的基础。
同时其他模态的特征可以作为动态视频特征的一个补充，所以研究内容第二点又可以嵌入到研究内容第三点以保证项目研究的完整性。
因此，本项目的研究工作具有系统性和整体性。


\textcolor{NsfcBlue}{\subsection{拟解决的关键科学问题}}

本项目以开放环境下的视觉感知为研究对象，旨在设计鲁棒的类脑感知方法以获得更好的跟踪精度，拟解决如下关键科学问题：

\subsubsection{图像刺激下皮层激活信息的精细化编解码模型}
面向开放环境的目标识别方法需要具有良好的识别适应性以应对复杂多变的实际场景。
然而，为了获得更好的识别性能，主流的识别方法（如：深度学习方法）往往会学习到高度复杂的识别模型。
高度复杂的跟踪模型不仅计算代价大，而且深度模型的“黑盒”效应导致模型的可解释性低。
因此，如何对皮层激活的视觉信息进行精细化编解码，
如何对识别模型进行类脑设计和相似性度量，
以提高识别模型的可解释性和适应性是提高本项目拟解决的关键科学问题之一，也是提升开放环境下的识别性能所必须要解决的困难。



\subsubsection{时间序列皮层激活信息解码策略}
面向开放环境的视觉目标跟踪方法需要具有良好的跟踪适应性以应对复杂多变的跟踪场景。
然而，为了获得更好的跟踪性能，主流的跟踪方法（如深度学习方法）往往会学习到高度复杂的跟踪模型。
高度复杂的跟踪模型不仅计算代价大，而且模型的可解释性差。
同时大脑的皮层激活的时间序列特征提取相比于静态的激活信息解码更具有挑战性。
因此，如何对高度复杂的跟踪模型进行精简以提高模型的跟踪可解释性和适应性是提高本项目拟解决的关键科学问题之一，也是提升开放环境下的跟踪性能所必须要解决的困难。

%目标在开放环境下不可避免地会出现光照变化、平面旋转、剧烈形变和运动模糊等多种外观变化，进而产生十分复杂的外观变化。
%复杂的外观变化大大地增加了跟踪方法成功跟踪目标的难度，降低了跟踪方法开放环境下的跟踪准确性。
%研究表明，单个视觉特征往往不能有效地捕获目标所出现的复杂外观变化。
%考虑到不同的视觉能够从不同的角度对目标进行表示，融合多种视觉特征便成为应对目标复杂外观变化的一种可行方案。
%因此，如何设计具有强表示能力的多特征融合以捕获目标复杂的外观变化也是本项目拟解决的关键科学问题。


\subsubsection{提出多模态融合类脑处理策略}
开放场景下智能感知拥有丰富的输入信息，包括视觉、听觉等，仅使用视觉信息可能导致不能有效利用其他模态的信息。
而一个鲁棒的视觉感知系统系统必须具有良好的鲁棒性来适应各种场景。
因此，本项目以视觉和听觉融合的角度出发，以探索听觉解码策略并设计听觉皮层解剖对齐的类脑听觉模型为基础，考虑单一模态感知能力不足的问题。
如何设计具有强鲁棒性的多模态融合的类脑感知框架以消除单一模态感知信息不足的问题是本项目拟解决的又一关键科学问题。


% TODO
\NsfcSection{3}{拟采取的研究方案及可行性分析}{（包括研究方法、技术路线、实验手段、关键技术等说明）；}

\textcolor{NsfcBlue}{\subsection{研究方法与技术路线}}
该课题主要包括三个阶段：类脑图像识别的设计分析和建模、类脑跟踪的机理研究和仿真、类脑感知融合探索和研究。
本项目结合理论研究与实际应用，针对开放环境下的目标跟踪所面临的感知可解释性问题、环境和目标动态变化、单一模态感知信息不足等挑战，
采用计算机视觉、深度学习、神经科学、类脑模型、全局工作空间理论、多模态融合等方法、技术与理论，考虑公开视据集、ADAS智能驾驶辅助系统和真实智能车平台相结合的验证与测试方法，
研究基于腹侧流解剖对齐的类脑精细化识别模型、基于腹侧流和背侧流融合的类脑视觉跟踪模型和基于大脑解剖对齐和全局工作空间的多模态融合模型，提高开放环境下的跟踪性能。

以开放环境下的视觉感知为研究对象，本项目的研究工作分为相关问题分析、模型算法设计和方法验证测试三个层面（图~\ref{fig:route}~为技术路线）：

（1）相关问题分析：利用当前主流的视觉感知系统和公开视视频和神经激活数据集，深入分析模型可解释性问题、目标动态变化问题和视觉感知信息不足等挑战对视觉感知算法性能和鲁棒性的影响。
此外，结合计算机视觉、深度学习、神经科学等相关理论方法与申请人前期关于视觉感知课题所做的研究工作，从类脑精细化识别、基于背侧流和腹侧流融合的跟踪、类脑多模态融合框架设计等方面展开具体的研究工作。
具体来说，从跟踪模型精简与压缩的角度研究跟踪适应性问题的解决方法、从特征融合的角度研究目标复杂的外观变化的解决方法、从跟踪框架设计的角度研究模型漂移问题的解决方法。

（2）模型算法设计：基于对关键性科学问题的详细分析与申请人在视觉感知、深度学习、神经科学、类脑模型、类脑相似性度量、多模态融合等方面的相关工作，设计鲁棒的类脑感知方法解决（1）中的科学问题以获得更好的感知效果。
其中，提出类脑精细化识别模型、类脑跟踪模型、类脑听觉感知模型解决模型可解释性问题，提出基于腹侧流和背侧流融合的类脑视觉跟踪模型捕获环境和目标动态变化的挑战，提出基于大脑皮层解剖对齐和全局工作空间的多模态融合模型应对单一模态感知信息不足的问题。

（3）方法验证测试：为有效地评估（2）中所设计的视觉感知方法，采用标准数据集、项目申请人参加研发的 ADAS 高级智能驾驶辅助系统与真实智能车平台进行测试和验证，分析智能感知算法的感知性能、优势和不足，增强感知算法在实际应用中的可行性。


综上所述，本项目基于\underline{大脑解剖对齐原理}，重点研究开放环境下的\underline{视觉感知}，
以\underline{模型可解释性}、\underline{环境和目标动态变化}、\underline{单一模态感知信息不足问题}等科学问题为驱动，
采用“\underline{\textbf{相关问题分析->模型算法设计->方法验证测试}}”的技术路线，研究“{基于腹侧流解剖对齐的类脑精细化识别模型}、{基于腹侧流和背侧流融合的类脑视觉跟踪模型}、{基于大脑皮层解剖对齐和全局工作空间的多模态融合模型}”三个方面的相关理论与关键技术。



\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/route"}
	}
	\caption{本项目整体研究思路与关系}
	\label{fig:route}
\end{figure*}


\textcolor{NsfcBlue}{\subsection{关键技术}}

\subsubsection{基于腹侧流解剖对齐的类脑精细化识别模型}

\textbf{1）类脑识别模型构建}

该研究设计的目标是在视觉深度神经网络（Deep Neural Networks，DNN）模型和大脑中的平滑跟踪通路之间获得高度的相似性，
同时遵循两个准则来设计类脑网络：

(1) \textbf{结构}：在性能相差不多的模型架构中，一般更加倾向于使用类脑模型，因为它模型复杂度低、更容易理解并且可以满足大脑的解剖约束。
使用深度神经网络是因为它们的神经元是数据处理的基本单元，并且深度神经网络中的所有神经激活都可以清晰地映射到大脑皮层激活。
除此之外，由于大脑皮层中广泛存在的循环连接，类脑视觉模型自然会考虑使用循环连接。
同时背侧和腹侧通路的激活也具有时间属性，因此也假定类脑模型会为时间序列生成激活响应。

(2) \textbf{功能}: 
在类脑模型中，满足神经解剖学约束的中间层激活响应和最终输出行为会有更准确的结果，
这使得设计的类脑模型能够更好地预测视觉皮层的激活和人的行为响应，
能同时满足计算机视觉目标跟踪预测和神经科学平滑跟踪预测的需求。

% 结构
图~\ref{fig:recognition_ch}~所示为基于大脑皮层解剖对齐的类脑跟踪模型技术架构。
根据大脑皮层视觉中腹侧流通路的研究，涉及目标识别的通路主要包括初级视觉皮层V1、纹外皮层 V2 和 V4、颞枕皮层 TEO、下颞皮层 IT，其中下颞皮层时目标识别的主要区域。
参照腹侧流通路的解剖结构，利用卷积和循环结构，构建大脑对齐的深度模型，可以设计相对应的类脑图像识别网络，并获得输入图像在深度网络中的激活响应，研究图像识别功能在大脑皮层和深度神经网络中表现的激活相似性。
\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/recognition_ch"}
	}
	\caption{ 基于大脑皮层解剖对齐的类脑跟踪模型的架构图}
	\label{fig:recognition_ch}
\end{figure*}

% 响应的编解码
大脑视觉信息的处理过程是一个高度非线性变化，可以采用两阶段的线性化编码方式。
如图~\ref{fig:encoding_model}所示，首先通过非线性变换实现自然图像空间到图像特征空间的映射，然后通过线性变换，完成图像特征空间（比如循环神经网络）到体素响应空间（下颞皮层）的映射。
\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/encoding_model"}
	}
	\caption{ 两阶段非线性视觉编码模型原理图}
	\label{fig:encoding_model}
\end{figure*}


\textbf{2）大脑数据分析}

在被试观看图像的过程中，采集被试的 fMRI 原始数据，然后使用 Statistical Parametric Mapping (SPM) 对原始数据进行三维重构、配准、头动校正等预处理操作，得到被试在观看不同图像刺激时对应的感兴趣视觉区域体素随时间变化的血氧水平依赖响应时间信号，反映了大脑对相应图像刺激的神经活动。
通常通过解卷积的方式得到血氧水平依赖响应信号的幅值，作为图像刺激对应的大脑视觉皮层体素的响应值。
一幅图像刺激对应的成千上万个体素的响应值组成了大脑视觉皮层对这一幅图像刺激的神经响应情况。
按照这种方式，可以得到每幅图像刺激对应的大脑视觉皮层体素响应向量，从而组成了最终的 fMRI 数据集。
另外，通常会选择多个被试进行相同的实验并采集和处理数据，从而包含多套被试的 fMRI 数据，用于后续验证计算模型在不同被试数据上的鲁棒性。
考虑到 fMRI 实验设计、数据采集及预处理等过程较为相似，本研究更加侧重通过预处理后的 fMRI 数据，针对自然图像刺激下的视觉信息解析任务设计符合视觉皮层信息表征特点的深度神经网络计算模型，分析和验证人类视觉信息处理机制和特征表达方式，探究深度神经网络与人类视觉信息处理的关系。
自然图像刺激下的 fMRI 视觉信息解析领域存在若干公开的 fMRI 实验数据库，这些数据被很多研究团队使用并公开发表大量相关研究工作，具有很高的可信度。
因此，本研究考虑到与前人实验结果进行比较，拟使用公开 fMRI 数据集进行类脑模型构建、验证、对比和分析。


\textbf{3）解码精度衡量}

% 解码
视觉分类模型首先在 fMRI 数据训练集上训练视觉皮层 fMRI 体素响应到图像场景语义类别的映射，然后，能够根据测试集中新的视觉皮层 fMRI 体素响应预测被试所观看图像刺激的场景语义类别。
通常视觉皮层中体素数量较多，即视觉皮层区域 fMRI 体素响应向量维度较高，且 fMRI 数据集规模较小，直接进行模型训练容易导致过拟合。
因此，在视觉皮层大量体素响应输入模型之前，通常从视觉区域中的大量体素中挑选一些重要的体素，降低用于后续视觉分类模型构建的输入维度，从而提高分类模型的准确性和鲁棒性。

拟采用基于体素激活的方法，将每个体素看作独立变量进行分析，通过对比每个体素与基线的激活响应程度，使用神经影像数据处理软件 SPM，根据一般线性模型(General Linear Model, GLM)，计算表征体素激活程度的系数，给每个体素进行打分。
最后按照得分从高到低的顺序选择一定数量的体素，也可以选择高于某个设定阈值的体素。

% 行为输出
同时进行进行神经度量和行为度量，利用类脑识别分数进行深度神经网络和大脑的对比，并利用设计并训练好的类脑模型，确定识别的图像模式分别在深度网络和大脑皮层中的位置，寻找图像识别的类别信息在深度网络中的表征模式与大脑中的激活特征之间的映射关系。




\subsubsection{基于背侧流解剖对齐的类脑平滑跟踪模型}

% 基于背侧流的类脑视觉跟踪模型

参照人类大脑皮层中背侧流通路进行类脑跟踪的机理研究和仿真，通过量化的大脑相似性分数，并利用所获得的皮层解剖知识来启发类脑跟踪模型的设计。

\textbf{1）类脑平滑跟踪模型设计}

如图~\ref{fig:tracking_ch}~所示，类脑跟踪模型包括映射到人脑的四个区域：初级视觉皮层，中颞和上颞内侧区、额叶视区和脑干/小脑。
初级视觉皮层使用经典的卷积层进行建模，执行预处理以减少数据大小。
中颞区和上颞内侧区使用动态滤波器网络进行建模，额叶视区使用循环神经网络进行建模。
对于最开始的输入刺激，神经网络的输出表示大脑对齐模型中的深度神经网络的激活响应和边界框，而大脑皮层在跟踪时的表示是大脑皮层的激活和眼睛注视的位置，同时类脑跟踪模型显示了计算机视觉跟踪性能与大脑跟踪响应之间的相似性关系。

为了比较模型，通过检查深度神经网络中各层的激活来构建到皮层的映射，以便能够很好地理解特定大脑皮层区域中的激活，理想情况下，这种大脑皮层激活不需要多余参数的类脑模型所预测得到，将会降低传统深度跟踪模型的冗余度。
因此类脑跟踪模型由卷积层、动态滤波网络、长短时记忆网络和全连接层四个神经网络模块组成，它们类比于大脑平滑跟踪皮层通路中的初级视觉皮层、中颞区和上颞内侧区、额叶视区、脑干/小脑，其中脑干/小脑是运动预测器，将额叶视区的输出转换为相应的运动响应。
这种明确的大脑分区思想是设计类脑跟踪模型重要的一步，并且致力于寻找更通用的网络结构。
整个模型包括在皮层区域没有差异的神经网络，以及各种改进类脑跟踪模型的连接。
并在此基础上对比激活和行为之间的相似性。

% 需要用jpg图像
\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/tracking_ch"}
	}
	\caption{基于大脑皮层解剖对齐的类脑跟踪模型的架构图}
	\label{fig:tracking_ch}
\end{figure*}


\textbf{2）大脑数据分析}

本研究拟使用 SPM12 工具进行核磁共振数据的分析。
首先对每个 fMRI 记录利用标准的处理流程进行预处理。
包括对每个会话的平均图像进行功能性数据对齐，并将对齐后的数据与大脑解剖的 T1 扫描进行配准，
然后将其正则化到 MNI 模板，再重采样到 $3 \times 3 \times 3$立方毫米的体素中。
最终，在脉冲的半峰全宽上使用 8 毫米高斯核进行平滑。

在 StudyForrest 数据集的记录中，将整部电影刺激分割成 8 个不同的视频片段，每个大约 15 分钟，在扫描仪中分别展示给每个受试。
在第一阶段的个体分析中，
为了建模整个《阿甘正传》电影，将 8 个记录会话组合到一个设计矩阵中，每一行表示一个电影片段。
在设计矩阵的每个会话中，从同一受试观察不同的电影片段的凝视中回归出一个平滑跟踪回归变量、一个眼跳回归变量和一个电影运动回归变量。
为了考虑受试之间血液动力学响应开始时刻和宽度的变化，沿着时间和离散度梯度方向使用标准血液动力学响应函数（Hemodynamic Response Function，HRF）。
除了以上三个回归变量，还使用了从预处理重对齐步骤中得到的六个头部运动分量作为用于校正的干扰因素回归变量。

为了和扫描仪重复时间相一致，眼动和运动回归变量建模成有 2 秒间隔的事件序列，因此每个事件都被表示成两个扫描之间变化的回归变量。
每个事件的幅值都根据 2 秒时间窗口内对应的眼动或运动数目进行调整，当和整体均值相同时对应的值为 0。
在回归变量建模中，如果没有创建两个之间的强烈的正相关或者负相关，就不可能同时建模注视和平滑跟踪。
为了使注视和平滑跟踪相互依赖更加清晰，可以考虑受试开始跟踪一个目标。
然后随着注视幅值的减少，平滑跟踪回归变量的幅值就会成比例增加。

对每个受试数据独立地应用广义线性模型后，使用每个回归变量血液动力学响应函数的幅值部分（为了计算和平滑跟踪相关的感兴趣部分进行对比，该处理过程横跨了对应 8 个电影片段的 8 个记录会话）。
这些对比包括眼动和运动主要响应的对比、平滑跟踪和眼跳之间的对比，以及眼动和运动的对比。
最后，在第二阶段受试之间核磁共振分析中，对有效受试的第一阶段对比结果进行单样本 $t$ 检验。


\textbf{3）平滑跟踪皮层神经预测性}

皮层神经预测性是指在源系统中（比如深度神经网络）评估给定输入图像 $X$，预测在目标系统（比如视觉区域 V1 和 MT/MST）中的激活响应。
作为输入，该度量方法的评估过程为：刺激 $\times$ 神经网络 = 激活响应，这里的神经网络可以是深度人工神经网络，也可以是灵长类的大脑皮层。
并且源神经网络（深度人工神经网络）可以使用线性变换映射到目标神经网络（灵长类皮层网络）：
\begin{equation}
	y = Xw + \epsilon \mbox{，}
\end{equation}
这里 $w$ 表示线性回归的权重，$\epsilon$ 表示皮层记录的噪声。

此外还展示了类脑跟踪网络中最具预测性的网络层和特定的模型区域。

这些关系被拟合为从深度神经网络到大脑皮层的映射，并用这个映射关系来预测人的大脑皮层对视频帧的响应。
由于大脑的激活响应数据维度远远大于神经网络模型激活数据的维度，所以利用主成分分析（Principal Component Analysis, PCA）将大脑激活维度压缩到指定的维度来进行程序运行加速和相关性分析，
并利用来自 MT/MST 的激活来进行拟合。 
最终使用皮尔逊相关系数 $s_r$ 来衡量最终人工神经网络模型和视觉运动皮层的神经相似性分数，如下所示：
\begin{equation}
	s_r=\frac{\sum_{i=1}^{n} (y_i-\bar{y}) (y_i^\prime - \bar{y}^\prime) }{\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2 (y_i^\prime - \bar{y}^\prime)^2 }}\mbox{，}
\end{equation}
其中 $y^\prime$ 是人工神经网络模型特定层的激活，$y$ 是人类大脑皮层特定区域的激活，$n$ 是深度神经网络中对应的特征维度，$\bar{y}$ 和 $\bar{y}^\prime$ 是所有神经响应值的中位数（使用中位数是因为响应通常是非正态分布）。



\subsubsection{基于腹侧流和背侧流融合的类脑视觉跟踪模型}
% 腹侧/背侧融合
% todo

类脑跟踪网络的处理流程如图~\ref{fig:tracking_pipeline} 所示，在平滑跟踪中发挥特定作用的大脑皮层区域用矩形框表示。
对于给定的输入帧 $F_t$ 和注意力参数 $a_t$，模仿空间注意力的中央凹从 $F_t$ 中选择包含目标的局部区域视图 $f_t$。
此外，在考虑外观特征 $\alpha_t$ 的基础上，使用包括初级视觉皮层 V1 的背侧和腹侧通路的外观选择器，获得跟踪目标更加精细的特征 $m_t$，并在 LSTM 中更新隐藏状态 $h_t$。
并将 FEF 的输出 $o_t$ 和背侧流输出 $d_t$ 合并输入到脑干/小脑模块中进行解码，
来预测下一帧的空间注意力 $a_{t+1}$ 、外观注意力 $\alpha_{t +1}$、眼位校正信号 $\Delta p$，甚至眼球运动信号 $\Delta p_t$。
所有黑色箭头表示信息在同一个时间步内流动，而灰色箭头代表不同时间步之间的连接。

\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/tracking_pipeline"}
	}
	\caption{类脑视觉目标跟踪的网络架构}
	\label{fig:tracking_pipeline}
\end{figure*}

\textbf{BTN 架构}

% 处理流程
首先简要介绍 BTN 的处理流程，如图~\ref{fig:tracking_pipeline} 所示，在平滑跟踪中发挥特定作用的大脑皮层区域用矩形框表示。
对于给定的输入帧 $F_t$ 和注意力参数 $a_t$，模仿空间注意力的中央凹从 $F_t$ 中选择包含目标的局部区域视图 $f_t$。
此外，在考虑外观特征 $\alpha_t$ 的基础上，使用包括初级视觉皮层 V1 的背侧和腹侧通路的外观选择器，获得跟踪目标更加精细的特征 $m_t$，并在 LSTM 中更新隐藏状态 $h_t$。
并将 FEF 的输出 $o_t$ 和背侧流输出 $d_t$ 合并输入到脑干/小脑模块中进行解码，
来预测下一帧的空间注意力 $a_{t+1}$ 、外观注意力 $\alpha_{t +1}$、眼位校正信号 $\Delta p$，甚至眼球运动信号 $\Delta p_t$。
所有黑色箭头表示信息在同一个时间步内流动，而灰色箭头代表不同时间步之间的连接。
下面详细介绍类脑跟踪架构的每个模块。

\textbf{视网膜和背侧/腹侧通路}

视网膜上的空间选择器为当前帧 $F_t$ 选择中央凹视图 $f_t$，它的输出被输入到两条相互连接的腹侧/背侧通路。
腹侧通路识别被跟踪的目标，而背侧通路学习被跟踪目标的运动特征。
腹侧/背侧通路中的外观注意力由当前帧自下而上的中心凹视图信息 $f_t$ 和前一帧自上而下的外观注意力信息 $\alpha_{t+1}$ 所驱动。
然而，中央凹的空间注意力只依赖于空间注意力信息 $a_{t+1}$，
在这种情况下，自下而上的信息仅具有局部影响并依赖于某个位置的显著性输入，
但自上而下的信息将全局特征结合到局部分析中。

(1) \textbf{视网膜}: 
在类脑模型的处理流程中，根据空间注意力机制进行建模。
对于输入帧 $F_t \in R^{W \times H}$ 形成矩阵 $M_t^x \in R^{w \times W}$ 和 $M_t^y \in R^{h \times H}$，
矩阵中的每一行都包含一个高斯分布。
高斯分布的位置和宽度决定了输入帧的哪些部分被选为视网膜中的中央凹视图 $f_t$。
因此，中央凹视图 $f_t \in R^{h \times w}$ 可以表示为：
\begin{equation}
	f_t = M_t^y F_t (M_t^x)^T \mbox{，}
\end{equation}
用连续矩阵行的分布中心、步长和方差来表示注意力。
与循环注意力跟踪类似，只有步长和分布中心是通过 LSTM 预测的，而方差依赖于步长。
该操作在估计较小的方差时避免了过度的偏差，有助于提高学习的速度。
此外，中央凹 $f_t$ 的大小依赖于具体的实验分析。

(2) \textbf{腹侧通路}: 
腹侧通路（包括初级视觉皮层 V1）将中央凹视图 $f_t$ 转换为固定维度的外观表征向量 $v_t$，其中包括被跟踪目标的空间特征和外观特征，
具体地网络结构依赖于具体的实验分析。
在本章的腹侧通路架构中，使用卷积神经网络实现初级视觉皮层 V1，这是腹侧通路和背侧通路共享的模块。
然而，通常使用若干个卷积层和最大池化层来实现模仿人类初级视觉皮层 V1。
在这以后，处理流程分为背侧通路和腹侧通路。
最后，通过卷积神经网络提取被跟踪目标的特征 $v_t$，以实现腹侧通路功能。

(3) \textbf{背侧通路}: 
背侧通路（包括 V1 和 MT/MST）用于提取运动特征，将显著目标 $d_t$ 和中央凹视图中的背景进行分割。
使用动态滤波网络建模背侧通路（MT/MST），用于处理中央凹视图前景和背景之间的空间关系。
FC$(\cdot)$ 表示全连接层，
所以 MT/MST 中基于外观表示 $\alpha_t$ 动态预测卷积滤波器 $\phi_t$ 为：
\begin{equation}
	\left\{ \phi _t ^i \right\}_{i=1}^N = \text{FC}(\alpha_t) \mbox{，}
\end{equation}
V1 输出的的特征经过具有 $N$ 个卷积层的非线性滤波器，
然后，将其输出传递给卷积核大小为 $1 \times 1$ 的卷积和 Sigmoid 激活函数，将特征转换为二维掩码 $d_t$。
$d_t$ 中的每一点都表示被所跟踪目标占据的概率。

背侧通路的位置图是基于腹侧通路抽取的目标表示，这模拟了视觉系统中的噪声抑制机制。
因为在当前帧没有跟踪目标的表征时，LSTM 中的目标表征不会被覆盖，所以使用这种机制可以处理遮挡和漂移的情况。
因此，腹侧和背侧通路的输出可以被整合为：
\begin{equation}
	m_t = \text{FC}(conc(v_t \odot d_t)) \mbox{，}
\end{equation}
其中 $\odot$ 表示通过特征掩码执行显著目标提取的哈达玛积，
$conc$ 表示将矩阵的所有行连接成向量的连接运算符。

\textbf{额叶视区}

所提出的类脑跟踪方法性能取决于估计下一帧中目标外观和位置的能力。
因此，它在很大程度上依赖于跟踪目标状态的预测。
而 LSTM 可以利用时空和外观特征，使所提出的模型能够处理遮挡和漂移的情况，例如跟踪目标被其他干扰物遮挡的情况。

如图~\ref{fig:tracking_pipeline} 所示，将 LSTM 模块命名为 FEF，利用输出误差来预测眼球运动。
在平滑跟踪中，FEF 区域接受来自腹侧/背侧流的输出 $m_t$，
精调后的目标表征 $m_t$ 用于更新 FEF 中的隐藏状态 $h_t$，
并在 FEF 中存在类似的预测动作。
当模型训练过程中，被跟踪目标和眼球运动之间的延迟减少时，输出误差趋向于零。
因此，LSTM 需要即使在没有图像输入的情况下依然能够推断眼球的运动。
\begin{equation} \label{equ:c2:LSTM}
h_t, o_t = \text{LSTM}(h_{t-1}, m_t) \mbox{。}
\end{equation}


\textbf{脑干和小脑}

脑干和小脑一起（包括脑桥核、绒球、小脑蚓体和前庭核）使用全连接层进行建模，
然后利用背侧流的输出 $d_t$ 和额叶视区的输出 $o_t$ 来估计注意力 $a_{t+1}$ 和外观 $\alpha_{t+1}$，
最后通过动眼神经生成眼球运动信号 $\Delta p_t$。
\begin{equation} \label{equ:c2:FC}
\Delta p_t, \Delta a_{t+1}, \alpha_{t+1} = \text{FC}(conc(d_t), o_t) \mbox{，}
\end{equation}
\begin{equation} \label{equ:c2:attention}
a_{t+1} = a_t + tanh(c) \Delta a_{t+1} \mbox{，}
\end{equation}
其中 $c$ 是一个可训练参数，用较小的值进行初始化以限制训练期间模型更新的大小。
方程~\ref{equ:c2:LSTM} 到 ~\ref{equ:c2:attention} 描述信息更新的过程。
通过累积注意力变化来计算眼睛注视的位置，
学习空间注意力来估计下一帧中被跟踪目标的位置，并预测在时间 $t$ 相对于注意力的注视位置 $p_t$。
\begin{equation}
p_t = a_t + \Delta p_t \mbox{。}
\end{equation}

此外，参考脑干和小脑都可以利用反向动力学控制器进行建模的发现和想法，
基于动力学的平滑跟踪模型，这里假设反向动力学控制器是完美的，因此眼球的运动信号可以表示为：
\begin{equation}
\Delta p_t = \Delta p \mbox{，}
\end{equation}
其中 $\Delta p$ 是眼球运动的低通滤波器。
基于这个假设，在该研究中不需要实现反向动力学控制器。


\textbf{2）平稳跟踪样本的提取}

实验中每个凝视样本都包含一个四元组：时间、显示器坐标系下的横坐标 $x$ 和纵坐标 $y$、眼跟踪质量的置信度估计。
因为数据集使用的是单眼跟踪，所以置信度为 1 表示好的眼跟踪，而 0 表示跟踪丢失。
经数据检查后发现在所有受试中跟踪丢失的比例一般在 $1.2\%$ 和 $16.7\%$ 之间，而受试 5 和受试 20 跟踪丢失特别明显，丢失比例分别是 $86.7\%$ 和 $39.0\%$，所以将他们的数据从分析中剔除。
%
对余下的凝视样本利用一种比较有代表性的基于密度的聚类算法 MOSP 进行分类以得到平稳跟踪样本。
该算法在手动标注的公共数据集上和几个最新的眼动检测算法相比有很好的分类性能。
MOSP 算法的另一个优势是在程序中使用了简单且容易理解的门限值，这样保证在使用过程中比较容易进行调整，而不像一些深度学习方法的超参数调整困难。
这一步 MOSP 取得较高的平滑跟踪检测精度（比如较低的误检）对后续的分析非常重要。

MOSP 算法包括两个步骤，第一步负责眼跳检测，先将眼跳剔除，第二步负责区分注视和平滑跟踪。
眼跳检测算法使用一个高速度门限值和一个低速度门限值。
眼跳检测器使用高速门限值进行初始化，然后往门限值两端进行扩展，直到小于低速门限值。
使用大于输入样本之间噪声的高速门限值使算法对噪声更加鲁棒。
MOSP 算法的第二步是进一步处理眼跳间的间隔，通过给几乎可以确定是注视的样本赋予一个注视的标签。
余下的样本就标记为平滑跟踪的候选结果，并在所有受试中进行处理。
然后，只有当凝视样本密度大于某个门限值时才使用基于密度的聚类算法 DBSCAN 进行聚类。
基于密度的聚类算法能很好地检测出平滑跟踪是因为平滑跟踪的的两个属性。
% 
首先，只有在给定时间内刺激出现运动时，平滑跟踪才会出现；并且视频刺激中每一帧移动的目标数目较少。
%
第二个特点是移动的目标能吸引受试的注意力（特别是在电影中），并且它们经常会被超过一个受试所跟踪。
平滑跟踪的这两个属性使算法能够把真实的平滑跟踪从漂移和类平滑跟踪中区分开来，这对于 fMRI 实验中的平稳跟踪样本的提取特别重要。
即使一些凝视样本被错误的标记成候选平滑跟踪，只要在当前时间的相同区域其他受试没有相同的模式，那么它也不会被标记成平滑跟踪，通过这些机制提高算法的鲁棒性对识别与处理平滑跟踪相关的大脑区域特别重要。

由于原始的 MOSP 算法是针对 GazeCom 数据集进行设计和优化的，在本实验中对一些参数与进行了调整。
因为增加凝视轨迹的数目会提高平滑跟踪检测算法的精度，所以同时使用了不在核磁共振扫描仪里的记录（获取难度较小，且相比于扫描仪记录有更少的噪声）和在扫描仪里的记录。
尽管刺激大小不同，对不在扫描仪里的记录使用单位观测角中所包含的像素数来进行按比例缩放；
所以使用两个数据集进行平滑跟踪片段检测会有更高的置信度。


\textbf{3）评价指标：类脑跟踪分数}

这一部分介绍了衡量深度神经网络模型与人类大脑皮层之间的相似性的类脑跟踪分数 BTS。
BTS 是在特定实验数据集上测试的指标，包括眼动行为预测性和皮层神经预测性两个指标。

为了获得关于大脑相似性的量化指标，提出了类脑跟踪分数，用于评估跟踪模型类脑预测能力的指标：
(a) 对于 StudyForrest 数据集的每个输入视频帧，预测人眼跟踪目标时的平均人眼运动；
(b) 在 StudyForrest 数据集上预测人类视觉区域 MT/MST 中大脑皮层位置对每个输入视频帧的平均激活响应。

%所有眼动的娇娇
为了在统一指标上评估类脑跟踪网络，该指标计算了眼动行为预测性指标和皮层神经皮层预测性指标的平均值。

\textbf{眼动行为预测性}

眼动行为度量的目的是衡量对于特定任务深度神经网络输出与人眼行为输出之间的相似性。
在人眼目标跟踪中，被试者的注意力是一个与人瞳孔有关的圆形范围。
因此，将模型呈现的行为模式（眼睛注视的位置和瞳孔的大小）建模为圆形，并且它不同于视觉目标跟踪中的长方形边界框。
同时，由于该工作不仅仅是为了提高计算机的跟踪性能，更主要目的是实现类人的智能；
如果类脑跟踪网络获得了更好的行为相似性，就可以很好地预测眼睛注视的位置和范围。
否则会导致深度神经网络获得了完美的边界框拟合，却无法获得良好的人眼行为预测效果。

使用的 12 个视频序列是在自然背景下有一个显著的目标在移动，每个视频序列大约 20 秒，并利用眼动仪记录下受试的注视范围来表示跟踪显著目标，
并使用 12 个图像序列中的 149 张图像的受试眼球运动和深度神经网络预测来分析和评估所提出的模型的眼动行为预测性能。
总共的 149 帧图像中，每一帧都作为深度神经网络的输入，用于预测眼睛的注视范围。
然后通过每一帧中眼睛注意力的范围来衡量这种眼动行为预测性。

目标跟踪深度神经网络的输出结果是所跟踪目标的边界框，而受试的注意力是一个包含中心坐标 $x$、$y$ 和半径的圆形范围。
因此将眼动预测性或行为评分建模为实际眼睛注意的圆形范围 $S_{roi}^a$ 与深度神经网络预测边界框 $S_{roi}^b$ 之间的交并比（Intersection over Union，IoU），
并将所有帧序列中的整体行为指标~$s_b$ 作为眼动预测分数：
\begin{equation}
	s_b=\frac{area(S_{roi}^a \cap S_{roi}^b) }  { area(S_{roi}^a \cup S_{roi}^b) } \mbox{。}
\end{equation}


为了全面衡量类脑跟踪网络的类脑性能，拟同时考虑 IoU 行为度量和 MT/MST 皮层度量。
下面给出的 BTS $s_{t}$ 是两个分数的平均值 ：
\begin{equation} \label{equ:score_btn}
	s_{t} = \frac{s_b + s_r}{2}\mbox{。}
\end{equation}
考虑到归一化可能会惩罚方差较小的分数，而应该平等对待两个分数对类脑跟踪分数的重要性程度，因此类脑跟踪分数的设计没有在各个分数大小上进行归一化。


同时时在类脑跟踪模型的基础之上分析模型的动态响应和大脑皮层对运动的响应之间的相似性或相关性。
并加入第一个研究的类脑图像识别模型，类比于大脑皮层中腹侧流和背侧流之间的相互连接和影响，进一步提高视觉皮层的建模精度，以便更好地适应真实和复杂的动态场景，提高类脑模型的跟踪精度和类脑的真实性。






% 基于全局工作空间理论
\subsubsection{基于大脑皮层解剖对齐和全局工作空间的多模态融合模型}
\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/auditory_ch"}
	}
	\caption{基于大脑皮层解剖对齐的类脑音频识别模型和多模态融合架构图}
	\label{fig:auditory}
\end{figure*}

在之前工作的基础之上，考虑大脑中其他模态的信息和视觉信息的相互作用，类比于人类感知中的“通感”。
考虑在人所接收的所有感知信息中，听觉信息是仅次于视觉信息的第二大感知源，两者占人所有感知信息的百分之九十以上，所以在建模类脑的多模态感知时主要考虑人类的听觉。
听觉识别通路主要包括初级听觉皮层A1、带状区Belt、伞状区PB、中颞/下颞区，并利用卷积神经网络和循环网络为基本结构，模拟人脑处理模式，构建大脑对齐的类脑模型，进行激活和行为之间的对比，包括大脑激活和神经网络激活之间的相似性、网络预测的音频类别和人类动作选择之间的相似性。

同时由于听觉识别的核心区域中颞/下颞区和第一阶段中的目标识别的下颞核心区有重叠部位，为视觉和听觉的融合处理提供了神经解剖基础，可以进一步在图像识别模型和音频识别模型的基础之上设计视觉/听觉融合模块，进行高层特征级别融合，有望进一步提高处理复杂环境输入的能力和提升类脑模型的预测能力。
% 《意识与脑》
% ~\cite{dehaene2001towards}
根据全局工作空间理论，在边缘区域，知觉是平行进行的：编码光线和声音的神经元可以同时被激活，而且不会互相干扰。
专业化的模块位于外围，适用于各种任务，比如：目标识别、检测、分割、语音识别、运动等，每个模块通过模块相关的隐藏空间连接到位于中心位置的全局工作空间。
通过以循环一致性目标进行训练，工作空间学会了以几乎无监督的方式在任意两个模块的隐空间表征中进行切换。
然而，在更高级的皮质体系中，它们会相互抑制，使得这些区域只允许存在激活神经元的单一整合状态，即单个“想法”。

在进行类脑模型的研究过程中，期待逐步建立一套科学合理的类脑评价指标用于评价所设计的类脑深度神经网络模型和人脑大脑处理信息的相似程度，在类脑图像识别中表现为图像分类和人进行图片识别时选择的相似性，在类脑跟踪中表现为深度神经网络输出的定位框和以人眼中央凹为中心的注意力范围，在感知融合中表现为在融合其他类型信息的复杂条件下模型的预测输出和人行为的相似性。该指标不仅衡量和模型的预测输出和人类行为之间的相似性，而且还衡量模型面对相同的输入刺激，各个中间层的激活响应和人类大脑皮层各个区域的激活（血液动力学响应或者脑电信号等）之间的相似性或相关性，实现从模型结构、皮层激活响应、人类行为等方面进行综合的类脑相似性解释。


\textcolor{NsfcBlue}{\subsection{实验手段}}
本项目主要采用标准数据集、ADAS高级智能驾驶辅助系统和真实智能车平台相结合的实验方法，分析拟提出的跟踪方法在开放环境下的跟踪性能。
实验主要包括基于标准数据集的验证、基于ADAS系统的测试和基于真实智能车平台的测试等三个方面。

\subsubsection{基于标准数据集的验证}
当前已有许多具有大规模的标准数据集被发布以推动视觉感知的发展，例如 ImageNet、OTB、VOT 等。
这些数据集都是在复杂多变的场景下所采集到的，十分具有挑战性。
以权威的 OTB 和 VOT 为例，OTB数据集由100个视频所组成，包含了58000多帧图片，视频集捕获了复杂场景下的行人、车辆、动物等多种目标，VOT数据主要用于一年一度的视觉目标跟踪竞赛，该数据集每年都会被重新标注和更新来提高数据集的准确性和竞赛难度，目前主要包括60个具有高难度的视频。
本项目申请人所在的实验室拥有由多台服务器和 PC 机所组成的计算平台，该计算平台上安装了Pytorch、Keras 和 Tensflow 等深度学习框架，下载了 ImageNet、OTB、VOT 等计算机视觉数据集，并配置了不同数据集的测试程序。
值得一提的是，由于不同数据集采用不同的性能指标以评估跟踪算法的结果，因此测试程序可以有效地验证跟踪算法在不同数据集上的跟踪精度。
通过在大规模的标准数据集进行综合性实验能够可靠地验证拟提出的视觉感知算法在开放环境下的跟踪性能。

同时使用《阿甘正传》电影作为动态开放自然环境的一种近似，原始数据集StudyForrest 包含受试观看电影时的核磁共振记录。
原始的《阿甘正传》电影包含的实际的 7 个电影片段。
这7 个部分连接起来并重新分割为8 个视频刺激段，每个视频段对应一个核磁共振记录。
时间戳参照《阿甘正传》2002年发行的DVD，PAL制式，DE103519SV， 每秒25帧，并按照“HH:MM:SS.FRAME”的格式给出。
该数据集包含15个受试，使用飞利浦Achieva dStream 核磁共振扫描仪记录下他们在观看好莱坞电影《阿甘正传》时的脑部激活数据和眼睛凝视数据，另外15 位不在核磁共振扫描仪中记录的受试仅仅记录下他们的凝视数据，这些不在核磁共振扫描仪中记录的数据仅仅用于提高平稳跟踪事件的检测精度。
使用液晶投影仪进行电影刺激的播放，受试在配有前反射镜的核磁共振扫描仪中进行电影的观看，并记录下观看电影时的脑部激活情况。凝视数据使用高频眼球跟踪仪EyeLink 1000 进行记录，在记录核磁共振扫描数据时，配有长焦镜头并以1000 次每秒的速度进行采样。
使用7 特斯拉的高精度核磁共振扫描仪，重复时间为 2 秒，体素大小为3 $\times$ 3 $\times$ 3立方毫米获取核磁共振记录。
此外，还获得了一套综合的辅助数据（弥散张量成像、磁敏感加权图像、血管造影）以及用于评估技术和生理噪声成分的测量数据。


\subsubsection{基于ADAS高级智能驾驶辅助系统的测试}
项目申请人在攻读博士学位期间作为团队的骨干成员参与了ADAS高级智能驾驶辅助系统的研发工作，主要负责环境感知模块的主要功能与相关算法的设计。
具体来说，ADAS 系统中的环境感知模块主要包括行人与车辆检测与跟踪、交通标志检测与跟踪和车道线检测等任务。
图~\ref{fig:adas}~展示了与本项目研究相关的行人与车辆检测与跟踪和交通标志检测与跟踪的定性结果。
因此，可以将本项目拟提出的目标跟踪算法移植到 ADAS 系统中，通过利用 ADAS 系统平台测试跟踪方法的性能。

\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/adas"}
	}
	\caption{ADAS 系统中行人与车辆和交通标志检测与跟踪结果}
	\label{fig:adas}
\end{figure*}


\subsubsection{基于真实智能车平台的测试}
本项目申请人所在的团队与长沙智能驾驶研究院存在合作关系，具有智能车一台。
智能车平台上装有摄像头、激光雷达、毫米波雷达和组合导航等多个传感设备。
图~\ref{fig:car}~展示了智能车平台和团队成员进行实车测试的情况。
因此，本项目具有在真实智能车平台上进行实车测试的条件，将来有可能利用该智能车平台验证本项目拟提出的类脑感知算法在开放环境下的感知性能。

\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/car"}
	}
	\caption{真实无人车平台和在线测试视频}
	\label{fig:car}
\end{figure*}


\textcolor{NsfcBlue}{\subsection{可行性分析}}
（1）\textbf{研究目标明确，技术路线清晰}。
本项目主要研究基于大脑皮层解剖对齐的类脑感知方法。
具体来说，以视觉感知所存在的模型可解释性、环境和目标动态变化和单一模态感知信息不足等问题为驱动，
基于“相关问题分析——模型算法设计——方法验证测试”的技术路线，利用深度学习、计算机视觉、神经科学、全局工作空间理论和多模态融合等方法与理论，
拟提出基于腹侧流解剖对齐的类脑精细化识别模型、基于腹侧流和背侧流融合的类脑视觉跟踪模型和基于大脑皮层解剖对齐和全局工作空间的多模态融合模型以提高开放环境下的视觉感知的适应性、准确性和鲁棒性，进而提高开放环境下的感知性能。

（2）\textbf{项目申请人具有一定的研究基础}。
如上所述，关于该课题三个阶段已有一定的研究基础，包括基本的类脑图像识别模型和评价指标，可以要在此基础上基于已有的人脸识别刺激数据和对应的多模型神经成像数据，利用人脸识别任务分析类脑模型和深度神经网络对相同图像刺激所产生的激活之间的相似性或关联关系，以及进一步分析负责人脸识别区的梭状回和负责目标识别区的下颞回之间的联系和区别。
本课题组已设计和测试类脑跟踪中背侧通路的建模和仿真，建立初步的运动感知模型，定量化验证中颞区和上颞内侧区和动态滤波网络激活的相似性，并在公开数据集上进行了仿真和验证，可以在此基础上加入图像识别的腹侧流模型，进一步研究背侧流和腹侧流共同作用下动态环境的建模和仿真。
类脑感知融合研究中已对类脑听觉皮层模型进行了初步建模，在音乐流派分类数据集合对应的核磁共振数据集上进行了响应分析，需要在此基础上分析深度模型激活和大脑皮层激活之间的相似性，并加入第一阶段腹侧流通路的建模，以进一步提高类脑模型的鲁棒性，提升模型的激活和行为的预测精度。
%
上述研究工作为本项目关于基于腹侧流解剖对齐的类脑精细化识别模型、基于腹侧流和背侧流融合的类脑视觉跟踪模型和基于大脑皮层解剖对齐和全局工作空间的多模态融合模型的研究奠定了良好的基础。
另外，项目申请人在攻读博士学位期间对类脑视觉感知进行了长期深入的研究，获取了大量主流跟踪方法的源代码，并且关注目标跟踪领域的最新发展趋势，这些也为研究工作的展开提供了巨大的帮助。

（3）\textbf{具有有效的实验方法和理想的实验平台}。
本项目主要利用标准数据集、ADAS高级智能驾驶辅助系统和真实无人车平台相结合的实验方法。
目前，项目组已收集了ImageNet、OTB、VOT 和 StudyForrest 等多个大规模标准数据集，并且为每个数据集配置了相应的测试程序以评估测试感知方法的性能。
项目申请人在攻读博士学位期间作为团队骨干成员参加了 ADAS 系统的研发工作，并且主要负责环境感知模块的功能与算法的设计。
因此，所研发的 ADAS 高级智能驾驶辅助系统能够用来进一步测试视觉感知方法的感知精度。
此外，申请人所在团队与长沙智能驾驶研究院存在合作，具有无人车平台，具备进行实车测试的可能。
项目申请人所在的湖南大学嵌入式与网络计算省重点实验室拥有由多台服务器和PC机所组成的强大计算平台，其中服务器配有多块 GeForce RTX 2080Ti 型号（共计 44GB 显存）、128G内存，这些是本项目研究工作开展与完成所必不可少的硬件条件。


\NsfcSection{4}{本项目的特色与创新之处；}{}

该类脑研究是可解释深度神经网络的一个很好地解决方案，设计大脑皮层对齐的类脑模型不仅可以设计出符合生物学解剖规律的深度神经网络，利用神经科学的原理来解释所设计的深度网络模型，有望解决深度神经网络“黑盒”的问题，还可以精简模型的层数，减少模型冗余，有利于工程化部署，加快计算机视觉算法的产业化落地。
同时利用设计并训练好的类脑深度神经网络探索脑机接口的应用和人产生视觉概念乃至意识的神经相关物，利用已经比较成熟的高性能计算机和深度学习算法，解决神经科学实验条件受限的不足，解决深度学习领域模型的复杂度越来越高、可解释性越来越差的问题。
本项目的特色与创新之处由主要包括如下几个方面：

\begin{itemize}
	\item 从模型可解释角度出发，将基于大脑皮层对齐的思想引入到类脑感知模型的设计之中，拟提出基于腹侧流解剖对齐的类脑精细化识别模型、基于腹侧流和背侧流融合的类脑跟踪模型、基于感知通路的类脑音频感知模型。
	这些模型不仅能够具有良好的识别性，而且能够利用大脑的生物学约束来限制模型的复杂度（即模型参数数目）以提高开放环境下的跟踪适应性；
	\item 研究类脑模型各层的激活和大脑皮层对应脑区激活之间的相似性，设计相似性度量标准全量衡量中间层激活和人类识别行为选择，判断所提出模型的有效性和鲁棒性；
	\item 从多模态融合框架设计的角度出发，拟提出基于大脑皮层解剖对齐和全局工作空间的多模态融合的感知方法。
	该方法基于全局工作空间将其他模态的信息融入视觉信息当中，强化视觉感知特征，可以有效地解决单一视觉感知信息不足的问题，提高开放环境下的感知的鲁棒性。
	\item 本项目具有明确的工程应用背景，相关科学问题能在重要工程领域得到应用，研究基础得到了实际工程验证，关键技术服务于工程关键技术领域突破，有利于推动人工智能产业科技进步。
\end{itemize}


\NsfcSection{5}{年度研究计划及预期研究结果}{（包括拟组织的重要学术交流活动、国际合作与交流计划等）。}

\textcolor{NsfcBlue}{\subsection{年度研究计划}}

（1）2023年1月-2023年12月

调研国际顶级会议与期刊中关于类脑视觉感知问题的最新研究工作与进展，把握领域前沿的研究趋势和热点，完善项目的主要目标整体规划，确定项目的具体工作；
针对视觉感知模型复杂度过高和可解释性不足所引起的感知适应性的问题，掌握模型类脑设计的主流方法，深入研究神经解剖对齐的类脑模型方法的特点与优势，将基于生物学约束引入到类脑模型中以构建相应的目标模型，训练合适的深度神经网络以对目标进行感知；
掌握基于类脑结构的卷积网络和循环神经网络的生物学连接意义，探索其在目标识别和视觉跟踪中的作用，尝试发现类脑模型和生物学机理之间更多的相关性。
撰写学术论文2\textasciitilde3篇，专利1\textasciitilde2个；

（2）2024年1月-2024年12月

了解主流的多模态融合和特征融合方法在开放环境下的感知性能、优势与不足；
研究不同模态信息的特性、优势和适应的感知场景，采用全局工作空间理论和多模态融合以建模目标外观，设计高效的多模态融合方法以设计目标模型；
在上述基础上，研究场景的多源信息（如深度信息，声音信息等），提出多模态信息融合方法以融合目标不同模态的特征；
撰写学术论文2\textasciitilde3篇，专利1\textasciitilde2个；

（3）2025年1月-2025年12月

深入分析不同跟多模态融合框架应对单一模态感知不足问题的鲁棒性，
研究全局共组空间理论的特点与性质，
设计基于全局工作空间理论的多模态融合方法，
提出基于多模态融合的类脑场景感知方法以消除单一模态感知能力不足的问题；
研究全局工作空间理论在感知理解、多模态融合、意识形成等方面的启发作用，以及尝试发现类脑感知模型中可能出现的意识相关物；
完善实验平台，验证和测试所提出的类脑感知方法的感知性能；
撰写学术论文2\textasciitilde3篇，软件著作权1\textasciitilde2个；
总结项目研究成果与相关研究资料，完成项目验收工作。


\textcolor{NsfcBlue}{\subsection{拟组织的重要学术交流活动、国际合作与交流计划}}

（1）参加1~2次人工智能和计算机视觉领域的顶级学术会议，如AAAI、CVPR、ICCV、ECCV和NIPS等；与相关领域的权威学者和学术团队进行学术问题的探讨与交流，掌握本领域的最新前沿研究方向和热点问题；

（2）参加1~2次国内计算机视觉和人工智能领域的重要学术会议，如中国模式识别与计算机视觉大会（PRCV）、CCF人工智能会议等；与国内本领域的专家与学术团队进行相关问题的讨论与交流，获取研究的新方向和新思路；

（3）计划在项目研究前期进行一次短期境外、国外访学，邀请国外研究专家（UC-Berkeley深度学习研究组、日本奈良先端科技研究院/东京大学视觉计算研究组）来华进行研讨与讲学，交流研究成果、研究方法和心得；


\textcolor{NsfcBlue}{\subsection{预期研究成果}}
（1）开发面向开放环境的类脑鲁棒视觉感知系统，该系统可以在复杂场景下达到理想的视觉感知适应性、准确性与鲁棒性；

（2）发表本领域SCI/EI高水平学术论文6\textasciitilde8篇，其中CCF A类/CCF B类/IEEE Transaction/SCI一区/SCI二区论文4\textasciitilde6篇，本领域其他高水平论文2\textasciitilde3篇；

（3）基于以上学术成果，申请相关专利2\textasciitilde3项与软件著作权1\textasciitilde2项；

（4）培养硕士研究生2\textasciitilde3名；



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NsfcChapter{（二）研究基础与工作条件}{}


\NsfcSection{1}{研究基础}{（与本项目相关的研究工作积累和已取得的研究工作成绩）；}

本项目申请人近年来研究了应用基于大脑解剖对齐的类脑建模方法、深度学习、神经科学、计算机视觉和多模态融合等方法以解决开放环境下的视觉感知的问题，相关研究工作已经发表于《ACM Transactions on Multimedia Computing Communications and Applications》（CCF B 类）、《Neurocomputing》（SCI 2区）、《Image and Vision Computing》（SCI 3区）、《Computer Vision and Image Understanding》（CCF B类）等国际著名期刊之上，具备了较好的研究基础。
这些研究工作为本项目研究的开展和深入奠定了理论基础和积累了宝贵经验。
另外，申请人还作为骨干成员参加了与相关工程项目的研发工作，这进一步为本项目的完成提供了实践基础。
具体来说，与本项目相关的研究基础主要包括如几个方面：

（1）视觉感知。本项目申请人在湖南大学攻读博士学位期间依托湖南大学嵌入式与网络计算湖南省重点实验室、国家超算（长沙）中心长期从事目标跟踪方面的相关研究，作为骨干参与了多项国家自然科学基金和高校企业合作的产学研项目，发表了多篇以目标跟踪为主题的高水平学术论文，具有相当丰富的学术研究与工程项目经验。
特别地，申请人作为主要参与人随团队完成了ADAS高级智能驾驶辅助系统的研发工作，主要负责了系统中环境感知模型的相关功能与算法的设计。
ADAS系统的环境感知模块包含了多个与目标跟踪有紧密联系的功能，这为本课题的研究工作提供了较好的工程基础。
另外，项目申请人完成了以目标跟踪为主题的博士学位论文，开发了一个具有良好跟踪性能且非常实的目标跟踪系统，提出了基于非局部注意力、类脑跟踪方法和基于多任务学习的感知方法以提高视觉感知的准确性和鲁棒性。
本项目是申请人博士期间所做研究的延续，也是对先前工作的进一步深化与扩展，这说明申请人具备了十分坚实的研究基础。


（2）基于神经解剖的类脑跟踪方法。
本项目申请人前期对基于大脑解剖的类脑跟踪模型建模方法进行了初步的研究，提出了类脑跟踪方法以学习具有和人类大脑皮层类似的跟踪模型，并在相同刺激下大脑皮层中发现了相似的激活模式，研究成果发表于《Neurocomputing》。
本项目旨在设计基于大脑皮层解剖对齐的跟踪模型并尝试基于类脑模型中发现和大脑皮层类似的激活，找到运动意识的物质基础。
因此，前期的研究工作为本项目的工作提供了研究基础。


（2）非局部注意力机制。
本项目申请人应用非局部注意力机制解决视觉感知任务，提出了基于非局部注意力机制的跟踪方法，研究成果发表于《Image and Vision Computing》和《Computer
Vision and Image Understanding》等国际期刊之上。
本项目旨在将大脑皮层解剖对齐和注意力的思想引入到类脑识别模型中以降低跟踪模型的复杂度，提高感知方法在开放环境下的适应性。
因此，前期研究工作为本项目的研究提供了重要的理论基础。

（3）多模态融合方法。
本项目申请人深入分析于研究了多模态融合和多任务学习方法，提出了联合检测和跟踪的的融合方法以有效地融合多种视觉形态来完成视觉感知任务，相关研究成果已经发表在《ACM Transactions on Multimedia
Computing Communications and Applications》期刊之上。
本项目旨在研究基于神经解剖对齐的多模态融合模型以捕获场景中中复杂的特征。
因此，前期相关研究工作对本项目具有重要的指导意义。


\NsfcSection{2}{工作条件}{（包括已具备的实验条件，尚缺少的实验条件和拟解决的途径，包括利用国家实验室、国家重点实验室和部门重点实验室等研究基地的计划与落实情况）；}

%项目依托单位为湖南工商大学，拥有“移动商务智能湖南省重点实验室”、“湖南省移动电子商务2011 协同创新中心”等，已具备良好的工作环境和实验条件。


项目依托单位为湖南工商大学，拥有“湘江实验室”、“数字经济时代的资源环境管理理论与应用国家基础科学中心”、“统计学习与智能计算湖南省重点实验室”、“生态环境大数据与智能决策技术湖南省工程研究中心”、“长沙人工智能社会实验室”等，已具备良好的工作环境和实验条件。
另外项目申请团队可访问湖南大学信息科学与工程学院、湖南大学嵌入式与网络计算实验室和超级计算与云计算研究所的计算资源，且可获得数据挖掘、人工智能、数据融合、神经科学、算法设计等各个方向的学术力量支持。

湖南大学嵌入式与网络计算实验室是信息科学与工程学院的三个省重点实验室之一，主要研究方向是智能感知、大数据计算、信息网络和嵌入式计算等，目前实验室包括教师28名、博士后3名、博士研究生20多名、硕士研究生100多名，近五年完成和主持多项国家自然科学基金重点项目、国家自然科学基金面上项目、国家“973”，“863”及重大科技支撑计划、高校与企业合作的产学研项目等。
实验室拥有由多台Dell服务器和PC机所构成的强大计算平台（图17（a）），其中服务器配有Core i9-9820X（3.30GHz）处理器、128G内存、多块GeForce RTX 2080Ti型号显卡（共计显存33G），PC机配有Core i7处理器、16G内存、显卡GTX730（显存2G）。该计算平台上配置了Keras、Tensflow和Caffe等深度学习框架，安装了Visual Studio、Matlab、Python等正版实验软件，除了需要对计算平台中的内存和显卡进行升级并且购买一些采集数据集的设备以外，不需要购买大型实验设备，具备完成本项目研究所需要的基本实验条件。


湖南大学国家级超级计算长沙中心（图~\ref{fig:compution_resource}（b）），在实验环境、科研基础及国际交流合作方面已具备良好的研究条件。
该中心是继天津和深圳之后，目前科技部正式批准建立的第三家国家级超级计算中心，工程总投资8.6 亿。中心采用“天河1A-HN”计算核心，一期工程主机为CPU 和GPU 异构结构，运算速度为每秒1372 万亿次，其中CPU 峰值性能为302 万亿次、GPU 峰值性能为1070 万亿次、总的计算核心超过3万，内存总容量为106TB、存储容量为1.5PB、系统综合功耗为1.212MW，建筑面积近3 万平方米。
超级计算机主机部分由国防科大负责研制、软件部分由国防科大和湖南大学共同负责，机器已于2011 年6 月上线运行。
现由湖南大学组建专门团队负责全面的运营管理，国防科大提供技术运营支撑。
一方面，国家超级计算长沙中心能够为本项目提供超强计算能力的实验平台；
另一方面，项目组已搭建了一个小规模的CPU+GPU工作站，该系统由 6 个计算节点组成，其中单纯多核CPU 计算节点共4 个，CPU+FPGA 计算节点共1 个，CPU+GPU 计算节点共1 个。
该平台为相关并行算法的研究提供了一个实验验证平台。

此外，申请人所在团队与长沙智能驾驶研究院建立了合作关系，具有无人车一辆，这也为本项目进行实车测试提供了可能。

\begin{figure*}[htb!]
	\centering  
	{\includegraphics[width=1.0\textwidth]{"images/compution_resource"}
	}
	\caption{本项目的计算资源}
	\label{fig:compution_resource}
\end{figure*}




\NsfcSection{3}{正在承担的与本项目相关的科研项目情况}{（申请人和项目组主要参与者正在承担的与本项目相关的科研项目情况，包括国家自然科学基金的项目和国家其他科技计划项目，要注明项目的名称和编号、经费来源、起止年月、与本项目的关系及负责的内容等）；}

无

\NsfcSection{4}{完成国家自然科学基金项目情况}{（对申请人负责的前一个已结题科学基金项目（项目名称及批准号）完成情况、后续研究进展及与本申请项目的关系加以详细说明。另附该已结题项目研究工作总结摘要（限500字）和相关成果的详细目录）。}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

无

\NsfcChapter{（三）其他需要说明的问题}{}

\NsfcSection{1}{}{申请人同年申请不同类型的国家自然科学基金项目情况（列明同年申请的其他项目的项目类型、项目名称信息，并说明与本项目之间的区别与联系）。}

无

\NsfcSection{2}{}{具有高级专业技术职务（职称）的申请人或者主要参与者是否存在同年申请或者参与申请国家自然科学基金项目的单位不一致的情况；如存在上述情况，列明所涉及人员的姓名，申请或参与申请的其他项目的项目类型、项目名称、单位名称、上述人员在该项目中是申请人还是参与者，并说明单位不一致原因。}

无

\NsfcSection{3}{}{具有高级专业技术职务（职称）的申请人或者主要参与者是否存在与正在承担的国家自然科学基金项目的单位不一致的情况；如存在上述情况，列明所涉及人员的姓名，正在承担项目的批准号、项目类型、项目名称、单位名称、起止年月，并说明单位不一致原因。}

无

\NsfcSection{4}{}{其他。}


\end{document}
